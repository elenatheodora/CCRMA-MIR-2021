{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTSIzKBhqmnY"
   },
   "source": [
    "# CCRMA MIR Workshop: MedleyDB and Neural Networks\n",
    "Elena Georgieva, Iran Roman\n",
    "\n",
    "In this assignment you will test different ML techniques to classify instruments. This assignment uses a large dataset (~8 GB) which you will download separately: *Medley-solos-DB*:\n",
    "\n",
    "<blockquote>\n",
    "V. Lostanlen, C.E. Cella. Deep convolutional networks on the pitch spiral for musical instrument recognition. Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016.\n",
    "</blockquote>\n",
    "\n",
    "Refer to https://scikit-learn.org/ for implementation instructions and tutorials.\n",
    "\n",
    "Some of this notebook was developed by Dirk Vander Wilt at NYU. Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K4Xkfjlcqmna"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from librosa import feature\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q17v1f_Kqmnc"
   },
   "source": [
    "## Part 1: Download Dataset and Metadata\n",
    "\n",
    "The data is a folder containing wav audio files, and a separate .csv file with metadata. You can download both here:\n",
    "\n",
    "https://zenodo.org/record/3464194#.X4G_oi2z3kJ\n",
    "\n",
    "Place both the folder and csv file into the same directory as your `CCRMA_MIR_MedleyDB.ipynb` file, such that the folder stucture is as follows:\n",
    "\n",
    "`\n",
    " \n",
    " <--   CCRMA_MIR_MedleyDB.ipynb\n",
    "\n",
    " <--   Medley-solos-DB_metadata.csv\n",
    "\n",
    " <--   Medley-solos-DB\n",
    "\n",
    "      <--   *.wav files\n",
    "`\n",
    "\n",
    "The audio files contain recordings from 8 different instuments which have already been labeled and separated into training, validation, and test sets. Each audio file is the same length, and there are many example files from each instrument.\n",
    "\n",
    "Each audio file has a unique id number associated with it ('uuid4'). This id is important when extracting the audio data and making sure that the file has the correct label, as referenced in the csv file. \n",
    "\n",
    "\n",
    "**1. Load the .csv file into a DataFrame called `Medley_Data`. Display the data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Dxp9REyeqmnd",
    "outputId": "ce8befc6-2bc7-4f42-a096-aa22cbd304d8"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, let's go and listen to a few .wav files. Is it easy for you to tell what instrument is performing in each one?**\n",
    "\n",
    "The list of instrument classes should be:\n",
    "\n",
    "0. clarinet\n",
    "\n",
    "1. distorted electric guitar\n",
    "\n",
    "2. female singer\n",
    "\n",
    "3. flute\n",
    "\n",
    "4. piano\n",
    "\n",
    "5. tenor saxophone\n",
    "\n",
    "6. trumpet\n",
    "\n",
    "7. violin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGgki-_Gqmnd"
   },
   "source": [
    "### Helper Function: `get_file_name_and_label()` and `get_ids()`\n",
    "\n",
    "The following helper functions have been provided for you.\n",
    "\n",
    "\n",
    "**1. Print out how many tracks are in each of the training, validation, and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-V1Q6A3Wqmne"
   },
   "outputs": [],
   "source": [
    "def get_file_name_and_label(uuid, path='Medley-solos-DB/', dataset=Medley_Data):\n",
    "    #Returns full file name and path from a uuid\n",
    "    rd = dataset.loc[ (dataset['uuid4'] == uuid) ]\n",
    "    file = path + 'Medley-solos-DB' + '_' + str(rd.values[0,0]) + '-'  + str(rd.values[0,2]) + '_' + rd.values[0,4] + '.wav'\n",
    "    label = rd.values[0,2]\n",
    "    return(file, label)\n",
    "                       \n",
    "def get_ids(subset, path = 'Medley-solos-DB/', dataset = Medley_Data):\n",
    "    #Get a np array of all uuids or a subset of files in the dataset \n",
    "    file_array = np.array([])\n",
    "    rd = dataset.loc[ (dataset['subset'] == subset) ]\n",
    "    if len(rd.index) < 1:\n",
    "        file_array = np.array([0])\n",
    "    else:\n",
    "        k = 0\n",
    "        for i in range(len(rd.index)):\n",
    "            file_array = np.append(file_array,rd.iloc[k,4])\n",
    "            k += 1\n",
    "    return(file_array)\n",
    "\n",
    "\n",
    "# Divides up file names into training, validation, and test sets\n",
    "tracks_train =  get_ids('training')\n",
    "tracks_validate = get_ids('validation')\n",
    "tracks_test = get_ids('test')\n",
    "\n",
    "# Your code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VzxXICQqmng"
   },
   "source": [
    "## Part 1a: Compute Features\n",
    "\n",
    "Create a function `compute_features()` such that the input is one audio file and the output is a single feature vector. This function should do the following:\n",
    "1. Load audio into a sample array.\n",
    "2. Compute the MFCCs of the input audio, and remove the first  coeficient.\n",
    "3. Compute the summary statistics of the MFCCs over time:\n",
    "    1. Find the mean and standard deviation for each feature (2 values for each feature)\n",
    "    2. stack these statistics into single 1-d vector of size (2*(n_mfccs-1))\n",
    "4. Return the 1-d vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Xo1Hf__Dqmng"
   },
   "outputs": [],
   "source": [
    "def compute_features(audiofile, n_fft=2048, hop_length=512, n_mels=128, n_mfcc=20):\n",
    "    ## Your Code Here\n",
    "    \n",
    "    return ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4mqpFUAqmnh",
    "scrolled": true
   },
   "source": [
    "## Part 1b: Create Feature Set - Provided\n",
    "\n",
    "Provided is a function `create_feature_set()` where the input is an array of audio files and output is a normalized feature set and an accompanying vector of class labels. \n",
    "1. We Iterate through all audio files in a list of uuids. The training, test, and validation lists have been created for you. For each uuid we:\n",
    "    1. Use `get_file_name_and_label()` to retrieve the audio file name and associated label\n",
    "    2. Use `compute_features()` to get the 1-d vector for that audio file.\n",
    "    3. Append the feature vector and label to their respective arrays, and continue to the next file.\n",
    "2. When finished, we output 2 numpy arrays: the feature matrix (n_samples, 2*(mfccs-1)) and the label (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WuaJyPR-qmnj"
   },
   "outputs": [],
   "source": [
    "def create_feature_set(id_list):\n",
    "    nTracks = len(id_list)\n",
    "    \n",
    "    features = np.zeros((nTracks, 38)) \n",
    "    labels = np.zeros(nTracks)\n",
    "    counter = 0\n",
    "    for id1 in id_list:\n",
    "        id1_filename, id1_label = get_file_name_and_label(id1)\n",
    "        id1_features = compute_features(id1_filename)\n",
    "        features[counter,:] = id1_features\n",
    "        labels[counter] = id1_label\n",
    "        counter +=1\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uE0_0g7vqmnj"
   },
   "source": [
    "## Part 2a: Get Mean and Standard Deviation\n",
    "\n",
    "** 1. Create a function `get_stats()` which gets the mean and standard deviation for each feature in the input matrix.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NbxjjQyJqmnj"
   },
   "outputs": [],
   "source": [
    "def get_stats(____):\n",
    "    # Your Code Here\n",
    "    return ___, ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKSxaPsfqmnk"
   },
   "source": [
    "### Getting Everything Ready\n",
    "\n",
    "The code in the following cell has been done for you. When all is well, run the code to compute features and training labels for the 3 data sets in Medley-solos-DB.\n",
    "\n",
    "Note, This code will take a while to run! It might be good to test on small subset of the data, like `tracks_train[0:500]`, just to make sure everything's working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QkkiGWfPqmnk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: (12236, 38)\n",
      "Training Set: (5841, 38)\n",
      "Validation Set: (3494, 38)\n"
     ]
    }
   ],
   "source": [
    "load_saved_tests = False # Change this to True if you want to load prevously-computed features\n",
    "\n",
    "if not load_saved_tests:\n",
    "    test_set, test_labels = create_feature_set(tracks_test)\n",
    "    print(\"Test Set: \" + str(test_set.shape))\n",
    "    train_set, train_labels = create_feature_set(tracks_train)\n",
    "    print(\"Training Set: \" + str(train_set.shape))\n",
    "    validate_set, validate_labels = create_feature_set(tracks_validate)\n",
    "    print(\"Validation Set: \" + str(validate_set.shape))\n",
    "    np.savetxt('test_set.csv', test_set, delimiter=',')\n",
    "    np.savetxt('test_labels.csv', test_labels, delimiter=',')\n",
    "    np.savetxt('train_set.csv', train_set, delimiter=',')\n",
    "    np.savetxt('train_labels.csv', train_labels, delimiter=',')\n",
    "    np.savetxt('validate_set.csv', validate_set, delimiter=',')\n",
    "    np.savetxt('validate_labels.csv', validate_labels, delimiter=',')\n",
    "else:\n",
    "    test_set = np.loadtxt('test_set.csv',delimiter=',')\n",
    "    test_labels = np.loadtxt('test_labels.csv',delimiter=',')\n",
    "    train_set = np.loadtxt('train_set.csv',delimiter=',')\n",
    "    train_labels = np.loadtxt('train_labels.csv',delimiter=',')\n",
    "    validate_set = np.loadtxt('validate_set.csv',delimiter=',')\n",
    "    validate_labels = np.loadtxt('validate_labels.csv',delimiter=',')\n",
    "    print(\"Test Set: \" + str(test_set.shape))\n",
    "    print(\"Training Set: \" + str(train_set.shape))\n",
    "    print(\"Validation Set: \" + str(validate_set.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGzfoY52qmnl"
   },
   "source": [
    "## Part 2b: Normalize Feature Sets\n",
    "\n",
    "**1. Using `get_stats()` find the mean and standard deviations for the training set. Then use those statistics to make all 3 data sets have a mean of 0 and standard deviation of 1.!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3jXtBojbqmnl"
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOar92KKqmnl"
   },
   "source": [
    "## Part 3: k-Nearest Neighbor\n",
    "\n",
    "Using the data from part 1, run a kNN classification experienment:\n",
    "\n",
    "- Use `sklearn` entirely\n",
    "- Run tests on the validation set with k = 1, 5, 20, and 50. What worked best?\n",
    "- When you decide on the best settings, run the experiment on the test set and output the f-measure and a confusion matrix. Hint: you can use the functions confusion_matrix() and f1_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x6lJTl8Dqmnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Now, we test on the Testing set: \n",
      "\n",
      "confusion matrix:\n",
      " [[ 152   27    9   34  280   19    4  207]\n",
      " [   0  814    1    0   71    8    0   61]\n",
      " [   0    5  696    0   57  118    0  266]\n",
      " [  75   85   92  402 1293   17   83 1120]\n",
      " [   0   31    2    1 2558    0    0   17]\n",
      " [   0   89    6    0  111   25   11   83]\n",
      " [  15    1    1    5   33   17  222  112]\n",
      " [  14  146    9   31  604    4    2 2090]]\n",
      "f1 score 0.5687316116377902\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OB6BmeKqmnn"
   },
   "source": [
    "## Part 4: Multi-Layered Perceptron (Neural Network)\n",
    "\n",
    "Multilayer Perceptrons, or MLPs for short, are the classic type of neural network.\n",
    "\n",
    "They are comprised of one or more layers of neurons. Data is fed to the input layer, there may be one or more hidden layers providing levels of abstraction, and predictions are made on the output layer.\n",
    "\n",
    "Using the same data, as aboverun the same test using the MLP classifier. Did it perform better?\n",
    "\n",
    "- Use `sklearn` entirely\n",
    "- Run tests on the validation set to experiment with the number of iterations and size and number of hidden layers.\n",
    "- Initially, try setting `max_iter=100` and `hidden_layer_sizes=(5,2)` (meaning 2 hidden layers of sizes 5 and 2.\n",
    "- When you decide on the best settings (best f-measure), run the experiment on the test set and output the f-measure and a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5T6g5aD6qmnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Now, we test on the Testing set: \n",
      "\n",
      "confusion matrix:\n",
      " [[ 283   25    9   22  241    2    9  141]\n",
      " [   0  842    1    0   80   14    0   18]\n",
      " [   0    1  787    0   30    9    1  314]\n",
      " [ 427  133  201  628 1102    1  249  426]\n",
      " [   0   12    0    0 2596    0    0    1]\n",
      " [   0  109    0    0  116   30   14   56]\n",
      " [  17    1    0    3   37    3  325   20]\n",
      " [   9   26    0   21   87    0    2 2755]]\n",
      "f1 score 0.6739130434782609\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJws1gjyqmnn"
   },
   "source": [
    "## Part 5: Discussion\n",
    "\n",
    "1. Which algorithm performed better?\n",
    "2. Which instrument class has the best & worst performance?\n",
    "3. Listen to the audio for examples the classifier got wrong. What do they have in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2QkakWds9nY"
   },
   "source": [
    "## Bonus\n",
    "\n",
    "What other algorithms might be interesting to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1erE14ZjvDZf"
   },
   "outputs": [],
   "source": [
    "## Your Code Here"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "CCRMA_MIR_MedleyDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
